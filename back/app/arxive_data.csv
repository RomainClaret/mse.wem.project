id,updated,published,title,summary,authors,affiliations,doi,journal_ref,pdf_link,primary_category,categories
http://arxiv.org/abs/1906.02739v1,2019-06-06T17:56:09Z,2019-06-06T17:56:09Z,Mesh R-CNN,"Rapid advances in 2D perception have led to systems that accurately detect objects in real-world images. However, these systems make predictions in 2D, ignoring the 3D structure of the world. Concurrently, advances in 3D shape prediction have mostly focused on synthetic benchmarks and isolated objects. We unify advances in these two areas. We propose a system that detects objects in real-world images and produces a triangle mesh giving the full 3D shape of each detected object. Our system, called Mesh R-CNN, augments Mask R-CNN with a mesh prediction branch that outputs meshes with varying topological structure by first predicting coarse voxel representations which are converted to meshes and refined with a graph convolution network operating over the mesh's vertices and edges. We validate our mesh prediction branch on ShapeNet, where we outperform prior work on single-image shape prediction. We then deploy our full Mesh R-CNN system on Pix3D, where we jointly detect objects and predict their 3D shapes.",Georgia Gkioxari|Jitendra Malik|Justin Johnson,,,,http://arxiv.org/pdf/1906.02739v1,cs.CV,cs.CV
http://arxiv.org/abs/1906.02738v1,2019-06-06T17:55:37Z,2019-06-06T17:55:37Z,"Conversing by Reading: Contentful Neural Conversation with On-demand
  Machine Reading","Although neural conversation models are effective in learning how to produce fluent responses, their primary challenge lies in knowing what to say to make the conversation contentful and non-vacuous. We present a new end-to-end approach to contentful neural conversation that jointly models response generation and on-demand machine reading. The key idea is to provide the conversation model with relevant long-form text on the fly as a source of external knowledge. The model performs QA-style reading comprehension on this text in response to each conversational turn, thereby allowing for more focused integration of external knowledge than has been possible in prior approaches. To support further research on knowledge-grounded conversation, we introduce a new large-scale conversation dataset grounded in external web pages 2.8M turns, 7.4M sentences of grounding). Both human evaluation and automated metrics show that our approach results in more contentful responses compared to a variety of previous methods, improving both the informativeness and diversity of generated output.",Lianhui Qin|Michel Galley|Chris Brockett|Xiaodong Liu|Xiang Gao|Bill Dolan|Yejin Choi|Jianfeng Gao,,,,http://arxiv.org/pdf/1906.02738v1,cs.CL,cs.CL|cs.AI|cs.LG
http://arxiv.org/abs/1906.02736v1,2019-06-06T17:55:17Z,2019-06-06T17:55:17Z,"DeepMDP: Learning Continuous Latent Space Models for Representation
  Learning","Many reinforcement learning RL) tasks provide the agent with high-dimensional observations that can be simplified into low-dimensional continuous states. To formalize this process, we introduce the concept of a DeepMDP, a parameterized latent space model that is trained via the minimization of two tractable losses: prediction of rewards and prediction of the distribution over next latent states. We show that the optimization of these objectives guarantees 1) the quality of the latent space as a representation of the state space and 2) the quality of the DeepMDP as a model of the environment. We connect these results to prior work in the bisimulation literature, and explore the use of a variety of metrics. Our theoretical findings are substantiated by the experimental result that a trained DeepMDP recovers the latent structure underlying high-dimensional observations on a synthetic environment. Finally, we show that learning a DeepMDP as an auxiliary task in the Atari 2600 domain leads to large performance improvements over model-free RL.",Carles Gelada|Saurabh Kumar|Jacob Buckman|Ofir Nachum|Marc G. Bellemare,,,,http://arxiv.org/pdf/1906.02736v1,cs.LG,cs.LG|stat.ML
http://arxiv.org/abs/1906.02735v1,2019-06-06T17:55:01Z,2019-06-06T17:55:01Z,Residual Flows for Invertible Generative Modeling,"Flow-based generative models parameterize probability distributions through an invertible transformation and can be trained by maximum likelihood. Invertible residual networks provide a flexible family of transformations where only Lipschitz conditions rather than strict architectural constraints are needed for enforcing invertibility. However, prior work trained invertible residual networks for density estimation by relying on biased log-density estimates whose bias increased with the network's expressiveness. We give a tractable unbiased estima1te of the log density, and reduce the memory required during training by a factor of ten. Furthermore, we improve invertible residual blocks by proposing the use of activation functions that avoid gradient saturation and generalizing the Lipschitz condition to induced mixed norms. The resulting approach, called Residual Flows, achieves state-of-the-art performance on density estimation amongst flow-based models, and outperforms networks that use coupling blocks at joint generative and discriminative modeling.",Ricky T. Q. Chen|Jens Behrmann|David Duvenaud|Jörn-Henrik Jacobsen,,,,http://arxiv.org/pdf/1906.02735v1,stat.ML,stat.ML|cs.LG
http://arxiv.org/abs/1906.02611v1,2019-06-06T17:54:24Z,2019-06-06T17:54:24Z,"Improving Robustness Without Sacrificing Accuracy with Patch Gaussian
  Augmentation","Deploying machine learning systems in the real world requires both high accuracy on clean data and robustness to naturally occurring corruptions. While architectural advances have led to improved accuracy, building robust models remains challenging. Prior work has argued that there is an inherent trade-off between robustness and accuracy, which is exemplified by standard data augment techniques such as Cutout, which improves clean accuracy but not robustness, and additive Gaussian noise, which improves robustness but hurts accuracy. To overcome this trade-off, we introduce Patch Gaussian, a simple augmentation scheme that adds noise to randomly selected patches in an input image. Models trained with Patch Gaussian achieve state of the art on the CIFAR-10 and ImageNetCommon Corruptions benchmarks while also improving accuracy on clean data. We find that this augmentation leads to reduced sensitivity to high frequency noisesimilar to Gaussian) while retaining the ability to take advantage of relevant high frequency information in the image similar to Cutout). Finally, we show that Patch Gaussian can be used in conjunction with other regularization methods and data augmentation policies such as AutoAugment, and improves performance on the COCO object detection benchmark.",Raphael Gontijo Lopes|Dong Yin|Ben Poole|Justin Gilmer|Ekin D. Cubuk,,,,http://arxiv.org/pdf/1906.02611v1,cs.LG,cs.LG|cs.CV|stat.ML
http://arxiv.org/abs/1906.02732v1,2019-06-06T17:51:51Z,2019-06-06T17:51:51Z,"A Look at the Effect of Sample Design on Generalization through the Lens
  of Spectral Analysis","This paper provides a general framework to study the effect of sampling properties of training data on the generalization error of the learned machine learning ML) models. Specifically, we propose a new spectral analysis of the generalization error, expressed in terms of the power spectra of the sampling pattern and the function involved. The framework is build in the Euclidean space using Fourier analysis and establishes a connection between some high dimensional geometric objects and optimal spectral form of different state-of-the-art sampling patterns. Subsequently, we estimate the expected error bounds and convergence rate of different state-of-the-art sampling patterns, as the number of samples and dimensions increase. We make several observations about generalization error which are valid irrespective of the approximation scheme or learning architecture) and training or optimization) algorithms. Our result also sheds light on ways to formulate design principles for constructing optimal sampling methods for particular problems.",Bhavya Kailkhura|Jayaraman J. Thiagarajan|Qunwei Li|Peer-Timo Bremer,,,,http://arxiv.org/pdf/1906.02732v1,cs.LG,cs.LG|stat.ML
http://arxiv.org/abs/1906.02729v1,2019-06-06T17:50:48Z,2019-06-06T17:50:48Z,3D-RelNet: Joint Object and Relational Network for 3D Prediction,"We propose an approach to predict the 3D shape and pose for the objects present in a scene. Existing learning based methods that pursue this goal make independent predictions per object, and do not leverage the relationships amongst them. We argue that reasoning about these relationships is crucial, and present an approach to incorporate these in a 3D prediction framework. In addition to independent per-object predictions, we predict pairwise relations in the form of relative 3D pose, and demonstrate that these can be easily incorporated to improve object level estimates. We report performance across different datasets SUNCG, NYUv2), and show that our approach significantly improves over independent prediction approaches while also outperforming alternate implicit reasoning methods.",Nilesh Kulkarni|Ishan Misra|Shubham Tulsiani|Abhinav Gupta,,,,http://arxiv.org/pdf/1906.02729v1,cs.CV,cs.CV
http://arxiv.org/abs/1906.02728v1,2019-06-06T17:49:41Z,2019-06-06T17:49:41Z,"Feature-level and Model-level Audiovisual Fusion for Emotion Recognition
  in the Wild","Emotion recognition plays an important role in human-computer interaction HCI) and has been extensively studied for decades. Although tremendous improvements have been achieved for posed expressions, recognizing human emotions in ""close-to-real-world"" environments remains a challenge. In this paper, we proposed two strategies to fuse information extracted from different modalities, i.e., audio and visual. Specifically, we utilized LBP-TOP, an ensemble of CNNs, and a bi-directional LSTM BLSTM) to extract features from the visual channel, and the OpenSmile toolkit to extract features from the audio channel. Two kinds of fusion methods, i,e., feature-level fusion and model-level fusion, were developed to utilize the information extracted from the two channels. Experimental results on the EmotiW2018 AFEW dataset have shown that the proposed fusion methods outperform the baseline methods significantly and achieve better or at least comparable performance compared with the state-of-the-art methods, where the model-level fusion performs better when one of the channels totally fails.",Jie Cai|Zibo Meng|Ahmed Shehab Khan|Zhiyuan Li|James O'Reilly|Shizhong Han|Ping Liu|Min Chen|Yan Tong,,,,http://arxiv.org/pdf/1906.02728v1,cs.CV,cs.CV
http://arxiv.org/abs/1906.02719v1,2019-06-06T17:39:48Z,2019-06-06T17:39:48Z,"Learning Gaussian Graphical Models with Ordered Weighted L1
  Regularization","We address the task of identifying densely connected subsets of multivariate Gaussian random variables within a graphical model framework. We propose two novel estimators based on the Ordered Weighted OWL) norm: 1) The Graphical OWL GOWL) is a penalized likelihood method that applies the OWL norm to the lower triangle components of the precision matrix. 2) The column-by-column Graphical OWL ccGOWL) estimates the precision matrix by performing OWL regularized linear regressions. Both methods can simultaneously identify highly correlated groups of variables and control the sparsity in the resulting precision matrix. We formulate GOWL such that it solves a composite optimization problem and establish that the estimator has a unique global solution. In addition, we prove sufficient grouping conditions for each column of the ccGOWL precision matrix estimate. We propose proximal descent algorithms to find the optimum for both estimators. For synthetic data where group structure is present, the ccGOWL estimator requires significantly reduced computation and achieves similar or greater accuracy than state-of-the-art estimators. Timing comparisons are presented and demonstrates the superior computational efficiency of the ccGOWL. We illustrate the grouping performance of the ccGOWL method on a cancer gene expression data set and an equities data set.",Cody Mazza-Anthony|Bogdan Mazoure|Mark Coates,,,,http://arxiv.org/pdf/1906.02719v1,stat.ML,stat.ML|cs.LG
http://arxiv.org/abs/1906.02717v1,2019-06-06T17:36:34Z,2019-06-06T17:36:34Z,Adaptive Gradient-Based Meta-Learning Methods,"We build a theoretical framework for understanding practical meta-learning methods that enables the integration of sophisticated formalizations of task-similarity with the extensive literature on online convex optimization and sequential prediction algorithms. Our approach enables the task-similarity to be learned adaptively, provides sharper transfer-risk bounds in the setting of statistical learning-to-learn, and leads to straightforward derivations of average-case regret bounds for efficient algorithms in settings where the task-environment changes dynamically or the tasks share a certain geometric structure. We use our theory to modify several popular meta-learning algorithms and improve their training and meta-test-time performance on standard problems in few-shot and federated deep learning.",Mikhail Khodak|Maria Florina-Balcan|Ameet Talwalkar,,,,http://arxiv.org/pdf/1906.02717v1,cs.LG,cs.LG|cs.AI|stat.ML
http://arxiv.org/abs/1905.13210v2,2019-06-06T17:34:56Z,2019-05-30T17:53:07Z,"Generalization Bounds of Stochastic Gradient Descent for Wide and Deep
  Neural Networks","We study the training and generalization of deep neural networks DNNs) in the over-parameterized regime, where the network width i.e., number of hidden nodes per layer) is much larger than the number of training data points. We show that, the expected - loss of a wide enough ReLU network trained with stochastic gradient descent SGD) and random initialization can be bounded by the training loss of a random feature model induced by the network gradient at initialization, which we call a neural tangent random feature NTRF) model. For data distributions that can be classified by NTRF model with sufficiently small error, our result yields a generalization error bound in the order of that is independent of the network width. Our result is more general and sharper than many existing generalization error bounds for over-parameterized neural networks. In addition, we establish a strong connection between our generalization error bound and the neural tangent kernel NTK) proposed in recent work.",Yuan Cao|Quanquan Gu,,,,http://arxiv.org/pdf/1905.13210v2,cs.LG,cs.LG|math.OC|stat.ML
http://arxiv.org/abs/1906.02715v1,2019-06-06T17:33:22Z,2019-06-06T17:33:22Z,Visualizing and Measuring the Geometry of BERT,"Transformer architectures show significant promise for natural language processing. Given that a single pretrained model can be fine-tuned to perform well on many different tasks, these networks appear to extract generally useful linguistic features. A natural question is how such networks represent this information internally. This paper describes qualitative and quantitative investigations of one particularly effective model, BERT. At a high level, linguistic features seem to be represented in separate semantic and syntactic subspaces. We find evidence of a fine-grained geometric representation of word senses. We also present empirical descriptions of syntactic representations in both attention matrices and individual word embeddings, as well as a mathematical argument to explain the geometry of these representations.",Andy Coenen|Emily Reif|Ann Yuan|Been Kim|Adam Pearce|Fernanda Viégas|Martin Wattenberg,,,,http://arxiv.org/pdf/1906.02715v1,cs.LG,cs.LG|cs.CL|stat.ML
http://arxiv.org/abs/1905.02706v2,2019-06-06T17:30:47Z,2019-05-07T17:45:22Z,"Learning Unsupervised Multi-View Stereopsis via Robust Photometric
  Consistency","We present a learning based approach for multi-view stereopsis MVS). While current deep MVS methods achieve impressive results, they crucially rely on ground-truth 3D training data, and acquisition of such precise 3D geometry for supervision is a major hurdle. Our framework instead leverages photometric consistency between multiple views as supervisory signal for learning depth prediction in a wide baseline MVS setup. However, naively applying photo consistency constraints is undesirable due to occlusion and lighting changes across views. To overcome this, we propose a robust loss formulation that: a) enforces first order consistency and b) for each point, selectively enforces consistency with some views, thus implicitly handling occlusions. We demonstrate our ability to learn MVS without 3D supervision using a real dataset, and show that each component of our proposed robust loss results in a significant improvement. We qualitatively observe that our reconstructions are often more complete than the acquired ground truth, further showing the merits of this approach. Lastly, our learned model generalizes to novel settings, and our approach allows adaptation of existing CNNs to datasets without ground-truth 3D by unsupervised finetuning. Project webpage: https://tejaskhot.github.io/unsup_mvs",Tejas Khot|Shubham Agrawal|Shubham Tulsiani|Christoph Mertz|Simon Lucey|Martial Hebert,,,,http://arxiv.org/pdf/1905.02706v2,cs.CV,cs.CV|cs.LG